---
title: "The GFLasso R package"
author:
      - Francisco de Abreu e Lima
      - Kris Sankaran
date: "2020-02-11"
output: rmarkdown::html_vignette   
vignette: >
  %\VignetteIndexEntry{Introduction to the GFLasso}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---
As machine learning techniques grow increasingly more sophisticated, high-dimensional multi-task problems that require the joint prediction of multiple responses remain largely understudied. This tutorial demonstrates the usage of the Graph-Guided Fused Lasso (GFLasso) in predicting multiple responses under a single regularized linear regression framework.

# Introduction

Most supervised learning models generally aim at predicting a single dependent variable (*i.e.* response) from a set of explanatory variables (*i.e.* predictors) over a set of samples (*i.e.* observations). Regularization methods introduce constraints that prevent overfitting of high-dimensional data, particularly when the number of predictors exceeds the number of observations. Such constraints, including the $L_1$ and the $L_2$ norms, are added to the objective function to set a limit over coefficient estimation, resulting in the shrinkage of weights associated with uninformative predictors. The least absolute shrinkage and selection operator (Lasso) [1] is one such method that employs the $L_1$ norm.

## What is the Lasso?
Compared to a ordinary least squares (OLS), the Lasso is capable of shrinking coefficient estimates ($\beta$) to exactly zero, thereby performing feature selection, via

$$argmin_\beta \sum_n(y_n-\hat{y_n})^2+\lambda\sum_{j}|\beta_{j}|$$

where *n* and *j* denote any given observation and predictor, respectively. The residual sum of squares (RSS), the sole term used in OLS, can equivalently be written in the algebraic form $RSS = \sum_n(y_n-\hat{y_n})^2 = (y-X\beta)^T.(y-X\beta)$. The Lasso penalty is $\lambda\sum_{j}|\beta_{j}|$, the $L1$ norm of the coefficient estimates weighted by $\lambda$.

## Why Graph-Guided Fused Lasso (GFLasso)?
While multiple independent Lasso models can effectively solve multi-task regression problems, the implicit use of response covariance in the model can theoretically increase prediction accuracy by borrowing information across responses, akin to a Bayesian formalism. This prediction coordination between responses is poised to counteract response-specific variation or noise. A good example is provided in the original GFLasso paper, whose authors resolve the associations between 34 genetic markers and 53 asthma traits in 543 patients [2].

## What is the GFLasso?
Let **X** be a matrix of size $n \times p$ , with $n$ observations and $p$ predictors and **Y** a matrix of size $n \times k$, with the same $n$ observations and $k$ responses, say, 1390 distinct electronics purchase records in 73 countries, to predict the ratings of 50 Netflix productions over all 73 countries. Models well poised for modeling pairs of high-dimensional datasets include orthogonal two-way partial least squares (O2PLS), canonical correlation analysis (CCA) and co-inertia analysis (CIA), all of which involving matrix decomposition [3]. Additionally, since these models are based on latent variables (*i.e.* projections based on the original predictors), the computational efficiency comes at a cost of interpretability. However, this trade-off does not always pay off, and can be reverted with the direct prediction of $k$ individual responses from selected features in **X**, in a unified regression framework that takes into account the relationships among the responses. Mathematically, the GFLasso borrows the regularization of the Lasso [1] discussed above and builds the model on the graph dependency structure underlying **Y**, as quantified by the $k \times k$ correlation matrix (*i.e.* the 'strength of association' aforementioned). As a result, similar (resp. dissimilar) responses will be explained by a similar (resp. dissimilar) subset of selected predictors. More formally, and following the notation used in the original manuscript [2], the objective function of the GFLasso is

$$argmin_\beta \sum_k(y_k- X\beta_k)^T.(y_k-X\beta_k)+\lambda\sum_{k}\sum_{j}|\beta_{jk}|+\gamma\sum_{(m,l)\in E}f(r_{ml})\sum_j|\beta_{jm}-sign(r_{ml})\beta_{jl}|$$

where, over all $k$ responses, $\sum_k(y_k- X\beta_k)^T.(y_k-X\beta_k)$ provides the RSS and $\lambda\sum_{k}\sum_{j}|\beta_{jk}|$ the regularization penalty borrowed from the Lasso, weighted by the parameter $\lambda$ and acting on the coefficients $\beta$ of every individual predictor $j$. The novelty of the GFLasso lies in $\gamma\sum_{(m,l)\in E}f(r_{ml})\sum_j|\beta_{jm}-sign(r_{ml})\beta_{jl}|$, the fusion penalty weighted by $\gamma$ that ensures the absolute difference between the coefficients $\beta_{jm}$ and $\beta_{jl}$, from any predictor $j$ and pair of responses $m$ and $l$, will be the smaller (resp. larger) the more positive (resp. more negative) their pairwise correlation, transformed or not, $f(r_{ml})$. This fusion penalty favours globally meaningful variation in the responses over noise from each of them. When the pairwise correlation is close to zero, it does nothing, in which case you are left with a pure Lasso. This underlying correlation structure of all $k$ responses, which can be represented as a weighted network structure, defaults to the absolute correlation, $f(r_{ml}) = |r_{ml}|$ but can be transformed to create GFLasso variants with any user-specified function, such as

1. Squared correlation, $f(r_{ml}) = r_{ml}^2$ (weighted)
2. Thresholded correlation, $f(r_{ml}) = \begin{cases} 1, & \mbox{if } r_{ml} > \tau \\ 0, & \mbox{otherwise} \end{cases}$ (unweighted)

with plenty more room for innovation. Although 2. is much less computationally intensive compared to 1. and the default absolute correlation [2], it does require a predefined cutoff, for example, $\tau = 0.8$.

To sum up, to fit a GFLasso model you will need a predictor matrix **X**, a response matrix **Y** and a correlation matrix portraying the strength of association between all pairs of responses in **Y**. Note that the GFLasso yields a $p \times k$ matrix of $\beta$, unlike the Lasso ($p \times 1$), and this coefficient matrix carries the associations between any given response $k$ and predictor $j$.

# Get started

The `gflasso` R package offers reproducible n-fold cross-validation with multi-threading borrowed from the `doParallel` package, and a couple of visualization tools. To run the GFLasso you will `devtools`, loading it and installing the `gflasso` package from the GitHub repository.

``` {r load, message = FALSE}
library(gflasso)
```

# Simulation

This simulation is outlined in the help page for the CV function `cv_gflasso`. By default, the CV computes the root mean squared error (RMSE) across a single repetition of a 5-fold CV, over all possible pairs between $\lambda \in \{0,0.1,0.2,...,0.9,1\}$ and $\gamma \in \{0,0.1,0.2,...,0.9,1\}$, the tuning grid. Note that user-provided error functions also work.

Besides the inherent statistical assumptions and speed performance, the choice of tuning grid ranges depends largely on mean-centering and unit-variance scaling all columns in `X` and `Y`. Note that mean-centering and scaling are not implemented in `gflasso`, so we strongly recommend centering and scaling `X` and `Y` beforehand, if necessary.

Briefly, the data is simulated from a random multivariate normal distribution `X` and a random multivariate normal `B` carrying interdependent coefficient sets, derived from the product of a random normally-distributed vector `u` by its transpose, plus Gaussian noise. Working backwards, `Y` is generated from the product between `X` and `B`, plus Gaussian noise.

The following example will derive the fusion penalty from an unweighted Pearson correlation-based network, with a cutoff of $r > 0.8$:

``` {r simulation, fig.width = 5, fig.height = 5, fig.align = "center"}
?cv_gflasso
set.seed(100)
X <- matrix(rnorm(100 * 10), 100, 10)
u <- matrix(rnorm(10), 10, 1)
B <- u %*% t(u) + matrix(rnorm(10 * 10, 0, 0.1), 10, 10)
Y <- X %*% B + matrix(rnorm(100 * 10), 100, 10)
R <- ifelse(cor(Y) > .8, 1, 0)
system.time(testCV <- cv_gflasso(scale(X), scale(Y), R, nCores = 1))
system.time(testCV <- cv_gflasso(scale(X), scale(Y), R, nCores = 2))
cv_plot_gflasso(testCV)
```

The optimal values of $\lambda$ (rows) and $\gamma$ (columns) that minimize the RMSE in this simulation, 0.6 and 1 respectively, do capture the simulated relationships. 

When a custom goodness-of-fit function `err_fun` is provided, you have to define whether to maximize or minimize the resulting metric using the argument `err_opt`. 

The next example aims at maximizing $R^2$ in the problem above, using a weighted association network with squared correlation coefficients (*i.e.* $f(r_{ml}) = r_{ml}^2$). In case more than two CPU cores are available, the user might want to increase `nCores` to speed up the analysis.

``` {r simulation2, fig.width = 5, fig.height = 5, fig.align = "center"}
# Write R2 function
R2 <- function(pred, y){
      cor(as.vector(pred), as.vector(y))**2
}

# X, u, B and Y are still in memory
R <- cor(Y)**2

# Change nCores if you have more than 2, re-run CV
testCV <- cv_gflasso(scale(X), scale(Y), R, nCores = 5, err_fun = R2, err_opt = "max")
cv_plot_gflasso(testCV)
```

The optimal hyperparameters $\lambda$ and $\gamma$ are now 0.3 and 0.2, respectively. 

The `cv_gflasso` output comprises a single list with four objects: the mean (`$mean`) and standard error (`$SE`) of the metric over all cells of the grid, the optimal $\lambda$ and $\gamma$ parameters (`$optimal`) and the name of the goodness-of-fit function (`$err_fun`). The cross-validated model from the present example clearly favors both sparsity ($\lambda$) and fusion ($\gamma$). 

You can fine-tune additional parameters, such as the Nesterov's gradient convergence threshold $\delta$ and the maximum number of iterations by passing `delta_conv` and `iter_max` to `additionalOpts`, respectively. Finally, for predicting new samples the `predict_gflasso` function can be used.

# Wrap-up

The GFLasso employs both regularization and fusion when modeling multiple responses, thereby facilitating the identification of associations between predictors ($X$) and responses ($Y$). It is best used when handling high-dimensional data from very few observations, since it is much slower than contending methods. Sparse conditional Gaussian graphical models [4] and Bayesian group-sparse multi-task regression model [5], for example, might be favoured chiefly for performance gains. Nevertheless, the GFLasso is highly interpretable. This GFLasso implementation was used in a omics-integrative approach to uncover new lipid genes in maize [6] and to relate single nucleotide polymorphisms to neuroimaging features from Alzheimer patients [7].

## References
1. Robert Tibshirani (1994). Regression shrinkage and selection via the Lasso. *Journal of the Royal Statistical Society*, 58, 267-288. 
2. Seyoung Kim, Kyung-Ah Sohn, Eric P. Xing (2009). A multivariate regression approach to association analysis of a quantitative trait network. *Bioinformatics*, 25, 12:i204–i212.
3. Chen Meng, Oana A. Zeleznik, Gerhard G. Thallinger, Bernhard Kuster, Amin M. Gholami, Aedín C. Culhane (2016). Dimension reduction techniques for the integrative analysis of multi-omics data. *Briefings in Bioinformatics*, 17, 4:628–641.
4. Lingxue Zhang, Seyoung Kim (2014). Learning Gene Networks under SNP Perturbations Using eQTL Datasets. *PLoS Comput Biol*, 10, 2:e1003420.
5. Keelin Greenlaw, Elena Szefer, Jinko Graham, Mary Lesperance, Farouk S. Nathoo (2017). A Bayesian group sparse multi-task regression model for imaging genetics. *Bioinformatics*, 33, 16:2513–2522.
6. Francisco de Abreu e Lima, Kun Li, Weiwei Wen, Jianbing Yan, Zoran Nikoloski, Lothar Willmitzer, Yariv Brotman (2018). Unraveling the lipid metabolism in maize with time-resolved multi-omics data. *The Plant Journal*, 93, 6:1102-1115.
7. Francisco de Abreu e Lima (2018). *GFLASSO: Graph-Guided Fused LASSO in R*. https://www.datacamp.com/community/tutorials/gflasso-R
